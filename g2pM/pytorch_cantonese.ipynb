{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "BOS_TOKEN = \"시\"\n",
    "EOS_TOKEN = \"끝\"\n",
    "SPLIT_TOKEN = \"▁\"\n",
    "\n",
    "def create_digest_cedict(mono_file, poly_file, output_file):\n",
    "    cedict = {}\n",
    "\n",
    "    with open(mono_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            char, pron = line.strip().split('\\t')\n",
    "            cedict[char] = [pron]\n",
    "\n",
    "    with open(poly_file, 'r', encoding='utf-8') as f:\n",
    "        temp_dict = {}\n",
    "        for line in f:\n",
    "            char, pron = line.strip().split('\\t')\n",
    "            if char not in temp_dict:\n",
    "                temp_dict[char] = []\n",
    "            temp_dict[char].append(pron)\n",
    "        \n",
    "        cedict.update(temp_dict)\n",
    "\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(cedict, f)\n",
    "\n",
    "create_digest_cedict('MONOPHONIC_CHARS.txt', 'POLYPHONIC_CHARS.txt', 'digest_cedict.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_char2idx(sent_files, output_file):\n",
    "    char2idx = {}\n",
    "    idx = 0\n",
    "\n",
    "    for sent_file in sent_files:\n",
    "        with open(sent_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                for char in line.strip():\n",
    "                    if char not in char2idx:\n",
    "                        char2idx[char] = idx\n",
    "                        idx += 1\n",
    "\n",
    "    char2idx[UNK_TOKEN] = idx\n",
    "    char2idx[PAD_TOKEN] = idx + 1\n",
    "    char2idx[BOS_TOKEN] = idx + 2\n",
    "    char2idx[EOS_TOKEN] = idx + 3\n",
    "\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump(char2idx, f)\n",
    "\n",
    "\n",
    "create_char2idx([\"train.sent\", \"dev.sent\", \"test.sent\"], \"char2idx.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_class2idx(lb_files, output_file):\n",
    "    class2idx = {}\n",
    "    idx = 0\n",
    "\n",
    "    for lb_file in lb_files:\n",
    "        with open(lb_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                for pron in line.strip().split():\n",
    "                    if pron not in class2idx:\n",
    "                        class2idx[pron] = idx\n",
    "                        idx += 1\n",
    "\n",
    "    class2idx[UNK_TOKEN] = idx\n",
    "    class2idx[PAD_TOKEN] = idx + 1\n",
    "\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump(class2idx, f)\n",
    "\n",
    "\n",
    "create_class2idx([\"train.lb\", \"dev.lb\", \"test.lb\"], \"class2idx.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def initialize_np_ckpt(char2idx, class2idx, embedding_dim=64, lstm_hidden_dim=32):\n",
    "    state_dict = {}\n",
    "\n",
    "    state_dict[\"embedding.weight\"] = np.random.randn(\n",
    "        len(char2idx), embedding_dim\n",
    "    ).astype(np.float32)\n",
    "\n",
    "    state_dict[\"lstm.weight_ih_l0\"] = np.random.randn(\n",
    "        4 * lstm_hidden_dim, embedding_dim\n",
    "    ).astype(np.float32)\n",
    "    state_dict[\"lstm.weight_hh_l0\"] = np.random.randn(\n",
    "        4 * lstm_hidden_dim, lstm_hidden_dim\n",
    "    ).astype(np.float32)\n",
    "    state_dict[\"lstm.bias_ih_l0\"] = np.zeros(4 * lstm_hidden_dim, dtype=np.float32)\n",
    "    state_dict[\"lstm.bias_hh_l0\"] = np.zeros(4 * lstm_hidden_dim, dtype=np.float32)\n",
    "\n",
    "    state_dict[\"lstm.weight_ih_l0_reverse\"] = np.random.randn(\n",
    "        4 * lstm_hidden_dim, embedding_dim\n",
    "    ).astype(np.float32)\n",
    "    state_dict[\"lstm.weight_hh_l0_reverse\"] = np.random.randn(\n",
    "        4 * lstm_hidden_dim, lstm_hidden_dim\n",
    "    ).astype(np.float32)\n",
    "    state_dict[\"lstm.bias_ih_l0_reverse\"] = np.zeros(\n",
    "        4 * lstm_hidden_dim, dtype=np.float32\n",
    "    )\n",
    "    state_dict[\"lstm.bias_hh_l0_reverse\"] = np.zeros(\n",
    "        4 * lstm_hidden_dim, dtype=np.float32\n",
    "    )\n",
    "\n",
    "    state_dict[\"logit_layer.0.weight\"] = np.random.randn(\n",
    "        lstm_hidden_dim, 2 * lstm_hidden_dim\n",
    "    ).astype(np.float32)\n",
    "    state_dict[\"logit_layer.0.bias\"] = np.zeros(lstm_hidden_dim, dtype=np.float32)\n",
    "    state_dict[\"logit_layer.2.weight\"] = np.random.randn(\n",
    "        len(class2idx), lstm_hidden_dim\n",
    "    ).astype(np.float32)\n",
    "    state_dict[\"logit_layer.2.bias\"] = np.zeros(len(class2idx), dtype=np.float32)\n",
    "\n",
    "    with open(\"np_ckpt.pkl\", \"wb\") as f:\n",
    "        pickle.dump(state_dict, f)\n",
    "\n",
    "\n",
    "char2idx = pickle.load(open(\"char2idx.pkl\", \"rb\"))\n",
    "class2idx = pickle.load(open(\"class2idx.pkl\", \"rb\"))\n",
    "\n",
    "initialize_np_ckpt(char2idx, class2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from g2pM2 import G2pM\n",
    "\n",
    "\n",
    "def load_data(sent_file, lb_file):\n",
    "    with open(sent_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        sentences = [line.strip() for line in f]\n",
    "    with open(lb_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        labels = [line.strip().split() for line in f]\n",
    "    return sentences, labels\n",
    "\n",
    "\n",
    "def prepare_data(sentences, labels, char2idx, class2idx):\n",
    "    input_ids = []\n",
    "    target_ids = []\n",
    "    target_indices = []\n",
    "    for sent, label in zip(sentences, labels):\n",
    "        input_id = [char2idx.get(char, char2idx[UNK_TOKEN]) for char in sent]\n",
    "        target_id = [class2idx.get(pron, class2idx[UNK_TOKEN]) for pron in label]\n",
    "        input_ids.append(input_id)\n",
    "        target_ids.append(target_id)\n",
    "\n",
    "        target_idx = [i for i, pron in enumerate(label) if pron in class2idx]\n",
    "        target_indices.append(target_idx)\n",
    "\n",
    "    max_length = max(len(seq) for seq in input_ids)\n",
    "    input_ids = [\n",
    "        seq + [char2idx[PAD_TOKEN]] * (max_length - len(seq)) for seq in input_ids\n",
    "    ]\n",
    "    target_ids = [\n",
    "        seq + [class2idx[PAD_TOKEN]] * (max_length - len(seq)) for seq in target_ids\n",
    "    ]\n",
    "\n",
    "    return np.array(input_ids), np.array(target_ids), target_indices\n",
    "\n",
    "\n",
    "def get_batches(data, batch_size):\n",
    "    inputs, targets, target_indices = data\n",
    "    for i in range(0, len(inputs), batch_size):\n",
    "        batch_inputs = inputs[i : i + batch_size]\n",
    "        batch_targets = targets[i : i + batch_size]\n",
    "        batch_target_indices = target_indices[i : i + batch_size]\n",
    "        yield np.array(batch_inputs), np.array(batch_targets), batch_target_indices\n",
    "\n",
    "\n",
    "def compute_loss(model, inputs, targets, target_indices):\n",
    "    lengths = np.sum(np.sign(inputs), axis=1)\n",
    "    max_length = max(lengths)\n",
    "\n",
    "    rev_seq = model.reverse_sequence(inputs, lengths)\n",
    "    fw_emb = model.get_embedding(inputs)\n",
    "    bw_emb = model.get_embedding(rev_seq)\n",
    "\n",
    "    fw_states, bw_states = None, None\n",
    "    fw_hs = []\n",
    "    bw_hs = []\n",
    "    for i in range(max_length):\n",
    "        fw_input = fw_emb[:, i, :]\n",
    "        bw_input = bw_emb[:, i, :]\n",
    "        fw_states = model.fw_lstm_cell(fw_input, fw_states)\n",
    "        bw_states = model.bw_lstm_cell(bw_input, bw_states)\n",
    "\n",
    "        fw_hs.append(fw_states[0])\n",
    "        bw_hs.append(bw_states[0])\n",
    "    fw_hiddens = np.stack(fw_hs, axis=1)\n",
    "    bw_hiddens = np.stack(bw_hs, axis=1)\n",
    "    bw_hiddens = model.reverse_sequence(bw_hiddens, lengths)\n",
    "\n",
    "    outputs = np.concatenate([fw_hiddens, bw_hiddens], axis=2)\n",
    "    batch_size = outputs.shape[0]\n",
    "    if batch_size == 1:\n",
    "        outputs = outputs.squeeze(axis=0)\n",
    "        target_hidden = outputs[target_indices[0]]\n",
    "    else:\n",
    "        target_hidden = []\n",
    "        for i in range(batch_size):\n",
    "            for idx in target_indices[i]:\n",
    "                target_hidden.append(outputs[i, idx])\n",
    "        target_hidden = np.array(target_hidden)\n",
    "\n",
    "    logits = model.fc_layer(target_hidden)\n",
    "\n",
    "    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "    softmax_probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "\n",
    "    total_targets = len(target_hidden)\n",
    "    target_classes = []\n",
    "    for i in range(batch_size):\n",
    "        for idx in target_indices[i]:\n",
    "            target_classes.append(targets[i, idx])\n",
    "    target_classes = np.array(target_classes)\n",
    "\n",
    "    target_probs = softmax_probs[np.arange(total_targets), target_classes]\n",
    "\n",
    "    loss = -np.log(target_probs + 1e-9)\n",
    "    loss = np.sum(loss) / total_targets\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def update_weights(model, grads, learning_rate, beta1, beta2, epsilon, t, m, v):\n",
    "    for param, grad in grads.items():\n",
    "        m[param] = beta1 * m[param] + (1 - beta1) * grad\n",
    "        v[param] = beta2 * v[param] + (1 - beta2) * (grad**2)\n",
    "        m_hat = m[param] / (1 - beta1**t)\n",
    "        v_hat = v[param] / (1 - beta2**t)\n",
    "        model.__dict__[param] -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "\n",
    "\n",
    "def save_model(model, output_file):\n",
    "    state_dict = {\n",
    "        \"embedding.weight\": model.embeddings,\n",
    "        \"lstm.weight_ih_l0\": model.weight_ih,\n",
    "        \"lstm.weight_hh_l0\": model.weight_hh,\n",
    "        \"lstm.bias_ih_l0\": model.bias_ih,\n",
    "        \"lstm.bias_hh_l0\": model.bias_hh,\n",
    "        \"lstm.weight_ih_l0_reverse\": model.weight_ih_reverse,\n",
    "        \"lstm.weight_hh_l0_reverse\": model.weight_hh_reverse,\n",
    "        \"lstm.bias_ih_l0_reverse\": model.bias_ih_reverse,\n",
    "        \"lstm.bias_hh_l0_reverse\": model.bias_hh_reverse,\n",
    "        \"logit_layer.0.weight\": model.hidden_weight_l0,\n",
    "        \"logit_layer.0.bias\": model.hidden_bias_l0,\n",
    "        \"logit_layer.2.weight\": model.hidden_weight_l1,\n",
    "        \"logit_layer.2.bias\": model.hidden_bias_l1,\n",
    "    }\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump(state_dict, f)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    UNK_TOKEN = \"<UNK>\"\n",
    "    PAD_TOKEN = \"<PAD>\"\n",
    "    BOS_TOKEN = \"시\"\n",
    "    EOS_TOKEN = \"끝\"\n",
    "    SPLIT_TOKEN = \"▁\"\n",
    "\n",
    "    model = G2pM()\n",
    "\n",
    "    train_sentences, train_labels = load_data(\"train.sent\", \"train.lb\")\n",
    "    dev_sentences, dev_labels = load_data(\"dev.sent\", \"dev.lb\")\n",
    "\n",
    "    char2idx = pickle.load(open(\"char2idx.pkl\", \"rb\"))\n",
    "    class2idx = pickle.load(open(\"class2idx.pkl\", \"rb\"))\n",
    "\n",
    "    train_data = prepare_data(train_sentences, train_labels, char2idx, class2idx)\n",
    "    dev_data = prepare_data(dev_sentences, dev_labels, char2idx, class2idx)\n",
    "\n",
    "    epochs = 5\n",
    "    batch_size = 32\n",
    "    learning_rate = 0.001\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    epsilon = 1e-8\n",
    "\n",
    "\n",
    "    t = 0\n",
    "    m = {\n",
    "        param: np.zeros_like(value)\n",
    "        for param, value in model.__dict__.items()\n",
    "        if isinstance(value, np.ndarray)\n",
    "    }\n",
    "    v = {\n",
    "        param: np.zeros_like(value)\n",
    "        for param, value in model.__dict__.items()\n",
    "        if isinstance(value, np.ndarray)\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        total_targets = 0\n",
    "        with tqdm(total=len(train_data[0]), desc=f\"Epoch {epoch+1}/{epochs}\") as pbar:\n",
    "            for inputs, targets, target_indices in get_batches(train_data, batch_size):\n",
    "                t += 1\n",
    "                loss = compute_loss(model, inputs, targets, target_indices)\n",
    "                train_loss += loss\n",
    "\n",
    "                grads = {\n",
    "                    param: np.zeros_like(value)\n",
    "                    for param, value in model.__dict__.items()\n",
    "                    if isinstance(value, np.ndarray)\n",
    "                }\n",
    "\n",
    "\n",
    "                update_weights(\n",
    "                    model, grads, learning_rate, beta1, beta2, epsilon, t, m, v\n",
    "                )\n",
    "\n",
    "                target_count = sum(len(indices) for indices in target_indices)\n",
    "                total_targets += target_count\n",
    "                pbar.update(len(inputs))\n",
    "                pbar.set_postfix(\n",
    "                    {\n",
    "                        \"Train Loss\": (\n",
    "                            train_loss / total_targets if total_targets > 0 else 0.0\n",
    "                        )\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        dev_loss = 0\n",
    "        total_dev_targets = 0\n",
    "        for inputs, targets, target_indices in get_batches(dev_data, batch_size):\n",
    "            loss = compute_loss(model, inputs, targets, target_indices)\n",
    "            dev_loss += loss\n",
    "            total_dev_targets += sum(len(indices) for indices in target_indices)\n",
    "\n",
    "        avg_train_loss = (\n",
    "            train_loss / total_targets if total_targets > 0 else float(\"inf\")\n",
    "        )\n",
    "        avg_dev_loss = (\n",
    "            dev_loss / total_dev_targets if total_dev_targets > 0 else float(\"inf\")\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss}, Dev Loss: {avg_dev_loss}\"\n",
    "        )\n",
    "\n",
    "    save_model(model, \"trained_np_ckpt.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
